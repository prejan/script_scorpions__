{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esyRndlx_YoN"
   },
   "outputs": [],
   "source": [
    "!pip install torchmetrics torch rasterio pandas einops tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YfO8UMVC9124"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "from einops import rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from torchmetrics import Accuracy, F1Score, JaccardIndex\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score, MulticlassJaccardIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Vq61bAU-dyd"
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('training.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4cDisAt-sBG"
   },
   "outputs": [],
   "source": [
    "class BurnScarDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Directory with all the images\n",
    "            transform (callable, optional): Optional transform to be applied on a sample\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Find all files in the directory\n",
    "        merged_files = glob.glob(os.path.join(root_dir, \"*_merged.tif\"))\n",
    "        if not merged_files:\n",
    "            raise ValueError(f\"No merged files found in {root_dir}\")\n",
    "\n",
    "        logger.info(f\"Found {len(merged_files)} merged files in {root_dir}\")\n",
    "\n",
    "        merged_mask_pairs = []\n",
    "        for merged_file in merged_files:\n",
    "            base_name = merged_file.replace(\"_merged.tif\", \"\")\n",
    "            mask_file = f\"{base_name}.mask.tif\"\n",
    "\n",
    "            if os.path.exists(mask_file):\n",
    "                # Verify file readability\n",
    "                try:\n",
    "                    with rasterio.open(merged_file) as src:\n",
    "                        merged_shape = src.shape\n",
    "                    with rasterio.open(mask_file) as src:\n",
    "                        mask_shape = src.shape\n",
    "\n",
    "                    if merged_shape == mask_shape:\n",
    "                        merged_mask_pairs.append({\n",
    "                            'merged': merged_file,\n",
    "                            'mask': mask_file\n",
    "                        })\n",
    "                    else:\n",
    "                        logger.warning(f\"Shape mismatch for pair {base_name}: {merged_shape} vs {mask_shape}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Could not read pair {base_name}: {str(e)}\")\n",
    "                    continue\n",
    "            else:\n",
    "                logger.warning(f\"No matching mask file for {merged_file}\")\n",
    "\n",
    "        if not merged_mask_pairs:\n",
    "            raise ValueError(f\"No valid image pairs found in {root_dir}\")\n",
    "\n",
    "        self.pairs = merged_mask_pairs\n",
    "        logger.info(f\"Successfully loaded {len(self.pairs)} valid image pairs\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "\n",
    "        try:\n",
    "            # Read and process image\n",
    "            with rasterio.open(pair['merged']) as src:\n",
    "                image = src.read().astype(np.float32)\n",
    "                # Normalize image\n",
    "                image = (image - image.mean()) / (image.std() + 1e-6)\n",
    "\n",
    "            with rasterio.open(pair['mask']) as src:\n",
    "                mask = src.read(1).astype(np.int64)\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            return torch.from_numpy(image), torch.from_numpy(mask)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading sample {idx}: {str(e)}\")\n",
    "            raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36GSjgrP-r2o"
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels=3, patch_size=16, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * mlp_ratio),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim * mlp_ratio, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "class TransformerSegmentation(nn.Module):\n",
    "    def __init__(self, in_channels=6, num_classes=2, patch_size=16, embed_dim=768,\n",
    "                 depth=12, num_heads=12, mlp_ratio=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(in_channels, patch_size, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, (512 // patch_size) ** 2, embed_dim))\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "        # Initialize position embeddings\n",
    "        nn.init.normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # Add position embedding\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Final norm and head\n",
    "        x = self.norm(x)\n",
    "        x = self.head(x)\n",
    "\n",
    "        # Reshape to image dimensions\n",
    "        x = x.reshape(B, H // 16, W // 16, -1)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.interpolate(x, size=(H, W), mode='bilinear', align_corners=False)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7GyZrsF-rtP"
   },
   "outputs": [],
   "source": [
    "def plot_training_curves(history, output_dir, epoch):\n",
    "    \"\"\"\n",
    "    Plot training curves for loss and various metrics\n",
    "    \"\"\"\n",
    "    # Create a figure with multiple subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f'Training Metrics - Epoch {epoch}')\n",
    "\n",
    "    # Plot Loss\n",
    "    axes[0, 0].plot(history['train_loss'], label='Train')\n",
    "    axes[0, 0].plot(history['val_loss'], label='Validation')\n",
    "    axes[0, 0].set_title('Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    # Extract metrics from history using updated keys\n",
    "    train_iou = [metrics['iou'] for metrics in history['train_metrics']]\n",
    "    val_iou = [metrics['iou'] for metrics in history['val_metrics']]\n",
    "\n",
    "    train_f1 = [metrics['f1_score'] for metrics in history['train_metrics']]\n",
    "    val_f1 = [metrics['f1_score'] for metrics in history['val_metrics']]\n",
    "\n",
    "    train_acc = [metrics['accuracy'] for metrics in history['train_metrics']]\n",
    "    val_acc = [metrics['accuracy'] for metrics in history['val_metrics']]\n",
    "\n",
    "    # Plot IoU\n",
    "    axes[0, 1].plot(train_iou, label='Train')\n",
    "    axes[0, 1].plot(val_iou, label='Validation')\n",
    "    axes[0, 1].set_title('IoU Score')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('IoU')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "\n",
    "    # Plot F1 Score\n",
    "    axes[1, 0].plot(train_f1, label='Train')\n",
    "    axes[1, 0].plot(val_f1, label='Validation')\n",
    "    axes[1, 0].set_title('F1 Score')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('F1')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "\n",
    "    # Plot Accuracy\n",
    "    axes[1, 1].plot(train_acc, label='Train')\n",
    "    axes[1, 1].plot(val_acc, label='Validation')\n",
    "    axes[1, 1].set_title('Accuracy')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Accuracy')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "\n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'training_curves_epoch_{epoch}.png'))\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPXmm2rH-rkK"
   },
   "outputs": [],
   "source": [
    "class MetricsCalculator:\n",
    "    def __init__(self, num_classes=3, device='mps'):\n",
    "        self.device = device\n",
    "        self.accuracy = MulticlassAccuracy(num_classes=num_classes, average='macro').to(device)\n",
    "        self.f1_score = MulticlassF1Score(num_classes=num_classes, average='macro').to(device)\n",
    "        self.iou = MulticlassJaccardIndex(num_classes=num_classes, average='macro').to(device)\n",
    "\n",
    "    def update(self, preds, target):\n",
    "        target = target + 1\n",
    "        # Ensure predictions are also remapped if necessary\n",
    "        preds = preds + 1\n",
    "        self.accuracy.update(preds, target)\n",
    "        self.f1_score.update(preds, target)\n",
    "        self.iou.update(preds, target)\n",
    "\n",
    "    def compute(self):\n",
    "        return {\n",
    "            'accuracy': self.accuracy.compute().item(),\n",
    "            'f1_score': self.f1_score.compute().item(),\n",
    "            'iou': self.iou.compute().item()\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        self.accuracy.reset()\n",
    "        self.f1_score.reset()\n",
    "        self.iou.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9SUbsl6-rQm"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
    "                num_epochs, device, output_dir):\n",
    "    \"\"\"\n",
    "    Training function with validation and metrics calculation\n",
    "    \"\"\"\n",
    "    best_val_iou = 0\n",
    "    history = {'train_loss': [], 'train_metrics': [], 'val_loss': [], 'val_metrics': []}\n",
    "\n",
    "    # Initialize metrics calculator\n",
    "    metrics_calc = MetricsCalculator(num_classes=3, device=device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        metrics_calc.reset()\n",
    "\n",
    "        train_iterator = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        for images, masks in train_iterator:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            predictions = outputs.argmax(dim=1)\n",
    "            metrics_calc.update(predictions, masks)\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_metrics = metrics_calc.compute()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        metrics_calc.reset()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                predictions = outputs.argmax(dim=1)\n",
    "                metrics_calc.update(predictions, masks)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_metrics = metrics_calc.compute()\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Log metrics\n",
    "        logger.info(f\"Epoch {epoch+1}\")\n",
    "        logger.info(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "        logger.info(\"Train Metrics:\")\n",
    "        for k, v in train_metrics.items():\n",
    "            logger.info(f\"{k}: {v:.4f}\")\n",
    "        logger.info(f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "        logger.info(\"Val Metrics:\")\n",
    "        for k, v in val_metrics.items():\n",
    "            logger.info(f\"{k}: {v:.4f}\")\n",
    "\n",
    "        # Update history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_metrics'].append(train_metrics)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_metrics'].append(val_metrics)\n",
    "\n",
    "        # Save best model\n",
    "        if val_metrics['iou'] > best_val_iou:\n",
    "            best_val_iou = val_metrics['iou']\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_metrics': val_metrics,\n",
    "                'history': history,\n",
    "            }, os.path.join(output_dir, 'best_model.pth'))\n",
    "\n",
    "            if val_metrics['iou'] > best_val_iou:\n",
    "                best_val_iou = val_metrics['iou']\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'val_metrics': val_metrics,\n",
    "                    'history': history,\n",
    "                }, os.path.join(output_dir, 'best_model.pth'))\n",
    "\n",
    "        # Plot training curves\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            plot_training_curves(history, output_dir, epoch + 1)\n",
    "\n",
    "    return model, history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0TLDuX8M_L4x"
   },
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Test function with detailed metrics calculation\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    metrics_calc = MetricsCalculator(num_classes=2, device=device)\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(test_loader, desc='Testing'):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            predictions = outputs.argmax(dim=1)\n",
    "            metrics_calc.update(predictions, masks)\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    test_metrics = metrics_calc.compute()\n",
    "    test_metrics['loss'] = avg_test_loss\n",
    "\n",
    "    # Print metrics in the requested format\n",
    "    print(\"┃**             Test metric             **┃**            DataLoader 0             **┃\")\n",
    "    print(\"┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\")\n",
    "    for key, value in test_metrics.items():\n",
    "        print(f\"│{f'test/{key}'.ljust(35)}│{f'{value:.16f}'.center(35)}│\")\n",
    "\n",
    "    return test_metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cO7dK2zC_QAM"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'batch_size': 8,\n",
    "        'num_epochs': 15,\n",
    "        'learning_rate': 1e-2,\n",
    "        'weight_decay': 0.01,\n",
    "        'patch_size': 16,\n",
    "        'embed_dim': 768,\n",
    "        'num_heads': 12,\n",
    "        'depth': 12,\n",
    "    }\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = f\"training_output_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(os.path.join(output_dir, 'training.log')),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    global logger\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device('mps')\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    try:\n",
    "        # Create datasets\n",
    "        train_dataset = BurnScarDataset(\n",
    "            '/Users/admin63/Python-Programs/Geospatial-Image-Stitcher-and-cloud-remover/custom-transformer-inferences/hls_burn_scars/training'\n",
    "        )\n",
    "        val_dataset = BurnScarDataset(\n",
    "            '/Users/admin63/Python-Programs/Geospatial-Image-Stitcher-and-cloud-remover/custom-transformer-inferences/hls_burn_scars/validation'\n",
    "        )\n",
    "\n",
    "        # Create data loaders with multiple workers\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        # Initialize model\n",
    "        model = TransformerSegmentation(\n",
    "            in_channels=6,\n",
    "            num_classes=2,\n",
    "            patch_size=config['patch_size'],\n",
    "            embed_dim=config['embed_dim'],\n",
    "            depth=config['depth'],\n",
    "            num_heads=config['num_heads']\n",
    "        ).to(device)\n",
    "\n",
    "        # Setup training\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay']\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=config['num_epochs']\n",
    "        )\n",
    "\n",
    "        # Train model\n",
    "        model, history = train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            num_epochs=config['num_epochs'],\n",
    "            device=device,\n",
    "            output_dir=output_dir\n",
    "        )\n",
    "\n",
    "        # Test model\n",
    "        logger.info(\"Starting model testing...\")\n",
    "        test_metrics = test_model(\n",
    "            model=model,\n",
    "            test_loader=val_loader,  # Using validation set for testing\n",
    "            criterion=criterion,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Save final model and metrics\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'history': history,\n",
    "            'test_metrics': test_metrics,\n",
    "            'config': config\n",
    "        }, os.path.join(output_dir, 'final_model.pth'))\n",
    "\n",
    "        logger.info(\"Training and testing completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
